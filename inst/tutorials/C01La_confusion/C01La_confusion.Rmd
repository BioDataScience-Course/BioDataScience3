---
title: "Matrices de confusion"
author: "Guyliann Engels & Philippe Grosjean"
description: "**SDD III Module 1** Matrices de confusion et métriques."
tutorial:
  id: "C01La_confusion"
  version: 3.0.0/5
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
BioDataScience3::learnr_setup()
SciViews::R()
# Required for RSConnect
# SciViews::R
library(rlang)
library(data.table)
library(ggplot2)
library(tibble)
library(tidyr)
library(dplyr)
library(dtplyr)
library(broom)
library(forcats)
library(collapse)
library(fs)
library(data.trame)
library(svFast)
library(svTidy)
library(svMisc)
library(svBase)
library(svFlow)
library(data.io)
library(chart)
library(tabularise)
library(SciViews)
# ... more
library(readxl)
library(testthat)
library(equatags)
```

```{r, echo=FALSE}
BioDataScience3::learnr_banner()
```

```{r, context="server"}
BioDataScience3::learnr_server(input, output, session)
```

------------------------------------------------------------------------

## Objectifs

Il est possible de créer une multitude de classifieurs différents à partir d'un même jeu de données. Afin de déterminer le classifieur le plus adapté, nous avons besoin d'évaluer la qualité de chacun d'eux. Pour ce faire, nous utilisons des métriques comme le taux de reconnaissance globale, la précision, le rappel.... La plupart de ces métriques se calculent sur base d'une matrice de confusion.

Ce tutoriel a pour objectifs :

-   Appréhender les matrices de confusion
-   Apprendre à choisir la bonne métrique
-   Calculer les principales métriques à partir d'une matrice de confusion 2 x 2

## Choix des métriques

```{r qu_metrics}
question("Quelle est la métrique la plus adaptée pour s'assurer que le classifieur trouve un maximum d'items d'une classe donnée ?",
  answer("Rappel", correct = TRUE,
    message = "Le rappel permet de connaitre quelle est la fraction de classe X trouvée par l'ordinateur parmi l'ensemble des items appartenant effectivement à cette classe."),
  answer("Précision",
    message = "La précision permet de connaitre quelle est la fraction que l'ordinateur a classé comme X et qui appartient effectivement à cette classe."),
  answer("Spécificité",
    message = "La spécificité est l'opposé du rappel. On s'intéresse aux vrais négatifs, donc, les items non classés comme X et qui n'en sont pas."),
  answer("Score F",
    message = "Il s'agit d'une métrique qui combine la précision et le rappel."),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  correct = "Vous avez trouvé la métrique la plus adaptée.",
  incorrect = "Attention, Ce n'est pas la bonne métrique.",
  submit_button = "Soumettre une réponse",
  try_again_button = "Resoumettre une réponse")
```

## Taux de reconnaissance global

Ces métriques peuvent sembler abstraites. En les calculant à la main, on peut les comprendre plus facilement. Intéressez-vous, pour débuter, au taux de reconnaissance globale.

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(250, 0, 10),
  B = c(0, 160, 90),
  C = c(0, 80, 180)) 
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la matrice de confusion fictive ci-dessus, calculez le taux de reconnaissance global du groupe B. Il s'agit de la première étape. Il faut définir les vrais positifs, les faux positifs, les faux négatifs et les vrais négatifs. Ensuite, il faut calculer la métrique d'intérêt.

```{r conf1_h2, exercise=TRUE}
tp <- ___ # vrai positif
fp <- ___ # faux positif
fn <- ___ # faux négatif
tn <- ___ # vrai négatif
# Calcul de la métrique
acc <- ___
acc
```

```{r conf1_h2-hint-1}
tp <- 160
fp <- 90
fn <- 80
tn <- 440
# Calcul de la métrique
acc <- ___
acc
## Attention, le prochain indice est la solution ##
```

```{r conf1_h2-solution}
## Solution ##
tp <- 160
fp <- 90
fn <- 80
tn <- 440
# Calcul de la métrique
acc <- (tp + tn) / (tp + fp + fn + tn)
acc
```

```{r conf1_h2-check}
#grade_result(
#  pass_if(~ identical(.result, ((160+440)/(160+90+80+440))), 
#    "Bien joué, c'est la somme des vrais positifs et négatifs sur le total général."
#    )
#)
grade_result(
  pass_if(~ identical(.result, ((160 + 440) / (160 + 90 + 80 + 440)))),
  correct = "C'est la somme des vrais positifs et négatifs sur le total général (en d'autres termes, la somme de la diagonale sur le total général).",
  incorrect =  "Révisez la formule mathématique qui permet de calculer le taux de reconnaissance global."
  )
```

## Taux de vrais positifs

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(90, 40, 10),
  B = c(0, 60, 90),
  C = c(0, 10, 140))
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la nouvelle matrice de confusion ci-dessus, calculez le **taux de vrais positifs** du groupe C.

```{r conf2_h2, exercise=TRUE}
tp <- ___
fp <- ___
fn <- ___
tn <- ___
# Calcul de la métrique
tpr <- ___
tpr
```

```{r conf2_h2-hint-1}
tp <- 140
fp <- 10
fn <- 100
tn <- 190
# Calcul de la métrique
tpr <- ___
tpr
## Attention, le prochain indice est la solution ##
```

```{r conf2_h2-solution}
## Solution ##
tp <- 140
fp <- 10
fn <- 100
tn <- 190
# Calcul de la métrique
tpr <- tp / (tp + fn)
tpr
```

```{r conf2_h2-check}
grade_result(
  pass_if(~ identical(.result, (140 / (140 + 100))),
    "La référence est l'ensemble des positifs, soit les vrais positifs, mais aussi les faux négatifs.")
)
```

## Spécificité

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(90, 40, 10),
  B = c(0, 60, 90),
  C = c(0, 10, 140))
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la matrice de confusion ci-dessus, calculez la **spécificité** du groupe A.

```{r conf3_h2, exercise=TRUE}
tp <- ___
fp <- ___
fn <- ___
tn <- ___
# Calcul de la métrique
specif <- ___
specif
```

```{r conf3_h2-hint-1}
tp <- 90
fp <- 50
fn <- 0
tn <- 300
# Calcul de la métrique
specif <- ___
specif
## Attention, le prochain indice est la solution ##
```

```{r conf3_h2-solution}
## Solution ##
tp <- 90
fp <- 50
fn <- 0
tn <- 300
# Calcul de la métrique
specif <- tn / (tn + fp)
specif
```

```{r conf3_h2-check}
grade_result(
  pass_if(~ identical(.result, (300 / (300 + 50))),
    "À l'inverse ici, on considère comme référence l'ensemble des négatifs, donc ceux qui ne sont pas du groupe A, c'est-à-dire les vrais négatifs additionnés des faux positifs.")
)
```

## Précision

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(100, 30, 20),
  B = c(0, 10, 150),
  C = c(30, 20, 110))
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la matrice de confusion ci-dessus, réalisez un dernier calcul. Quantifiez la **précision** du groupe B.

```{r conf4_h2, exercise=TRUE}
tp <- ___
fp <- ___
fn <- ___
tn <- ___
# Calcul de la métrique
prec <- ___
prec
```

```{r conf4_h2-hint-1}
tp <- 10
fp <- 150
fn <- 30
tn <- 260
# Calcul de la métrique
prec <- ___
prec
## Attention, le prochain indice est la solution ##
```

```{r conf4_h2-solution}
## Solution ##
tp <- 10
fp <- 150
fn <- 30
tn <- 260
# Calcul de la métrique
prec <- tp / (tp + fp)
prec
```

```{r conf4_h2-check}
grade_result(
  pass_if(~ identical(.result, (10 / (10 + 150))),
    "Ne pas se tromper car cette fois la référence est l'ensemble des items classés par l'ordinateur comme B, soit les vrais et les faux positifs.")
)
```

## Conclusion

Ces calculs de métriques devraient vous avoir permis de les comprendre un peu mieux. Ce sont des éléments cruciaux dans l'évaluation d'un classifieur. À chaque fois que vous devrez déterminer la qualité d'un classifieur, commencez par définir les métriques les plus pertinentes par rapport à vos objectifs. Dans les exercices que vous ferez plus loin, vous utiliserez ces métriques pour évaluer les performances de vos classifieurs et vous vous rendrez compte de leur intérêt en pratique.

```{r comm_noscore, echo=FALSE}
question_text(
  "Laissez-nous vos impressions sur cet outil pédagogique",
  answer("", TRUE, message = "Pas de commentaires... C'est bien aussi."),
  incorrect = "Vos commentaires sont enregistrés.",
  placeholder = "Entrez vos commentaires ici...",
  allow_retry = TRUE,
  submit_button = "Soumettre une réponse",
  try_again_button = "Resoumettre une réponse"
)
```
