---
title: "Matrices de confusion"
author: "Guyliann Engels & Philippe Grosjean"
description: "**SDD III Module 1** Matrices de confusion et métriques qui en découlent."
tutorial:
  id: "C01La_confusion"
  version: 2.0.0/4
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
BioDataScience3::learnr_setup()
SciViews::R()
```

```{r, echo=FALSE}
BioDataScience3::learnr_banner()
```

```{r, context="server"}
BioDataScience3::learnr_server(input, output, session)
```

------------------------------------------------------------------------

## Objectifs

-   Appréhender les matrices de confusion.
-   Être capable de calculer à la main les principales métriques à partir d'une matrice de confusion 2 x 2.

## Les métriques sur les matrice de confusion

### La taux de reconnaissance global

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(250, 0, 10),
  B = c(0, 160, 90),
  C = c(0, 80, 180)) 
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la matrice de confusion ci-dessus, calculez le taux de reconnaissance global du groupe B.

```{r conf1_h2, exercise = TRUE}
tp <- _)__ # TRUE POSITIVE, vrai positif
fp <- ___ # FALSE POSITIVE, faux positif
fn <- ___ # FALSE NEGATIVE, faux négatif
tn <- ___ # TRUE NEGATIVE, vrai négatif
# calcul de la métrique
conf <- ___
conf
```

```{r conf1_h2-hint-1}
tp <- 160
fp <- 90
fn <- 80
tn <- 440
# calcul de la métrique
conf <- ___
conf
## Attention, le prochain indice est la solution ##
```

```{r conf1_h2-solution}
## Solution ##
tp <- 160
fp <- 90
fn <- 80
tn <- 440
# calcul de la métrique
conf <- (tp + tn) / (tp + fp + fn + tn)
conf
```

```{r conf1_h2-check}
grade_result(
  pass_if(~ identical(.result, ((160+440)/(160+90+80+440))), "Oui, c'est la somme des vrais positifs et négatifs sur le total général.")
)
```

### La taux de vrai positif

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(90, 40, 10),
  B = c(0, 60, 90),
  C = c(0, 10, 140))
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la matrice de confusion ci-dessus, calculez le **taux de vrai positifs** du groupe C.

```{r conf2_h2, exercise = TRUE}
tp <- ___
fp <- ___
fn <- ___
tn <- ___
# calcul de la métrique
conf <- ___
conf
```

```{r conf2_h2-hint-1}
tp <- 140
fp <- 10
fn <- 100
tn <- 190
# calcul de la métrique
conf <- ___
conf
## Attention, le prochain indice est la solution ##
```

```{r conf2_h2-solution}
## Solution ##
tp <- 140
fp <- 10
fn <- 100
tn <- 190
# calcul de la métrique
conf <- tp / (tp + fn)
conf
```

```{r conf2_h2-check}
grade_result(
  pass_if(~ identical(.result, (140/(140 + 100))), "La référence est l'ensemble des positifs, soit les vrais positifs, mais aussi les faux négatifs.")
)
```

### La spécificité

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(90, 40, 10),
  B = c(0, 60, 90),
  C = c(0, 10, 140))
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la matrice de confusion ci-dessus, calculez la **spécificité** du groupe A.

```{r conf3_h2, exercise = TRUE}
tp <- ___
fp <- ___
fn <- ___
tn <- ___
# calcul de la métrique
conf <- ___
conf
```

```{r conf3_h2-hint-1}
tp <- 90
fp <- 50
fn <- 0
tn <- 300
# calcul de la métrique
conf <- ___
conf
## Attention, le prochain indice est la solution ##
```

```{r conf3_h2-solution}
## Solution ##
tp <- 90
fp <- 50
fn <- 0
tn <- 300
# calcul de la métrique
conf <- tn / (tn + fp)
conf
```

```{r conf3_h2-check}
grade_result(
  pass_if(~ identical(.result, (300/(300 + 50))), "À l'inverse ici, on considère comme référence l'ensemble des négatifs qui ne sont pas du groupe A, donc les vrais négatifs mais aussi les faux positifs.")
)
```

### La précision

```{r, echo=FALSE, message=FALSE}
mconf <- dtf(
  A = c(100, 30, 20),
  B = c(0, 10, 150),
  C = c(30, 20, 110))
rownames(mconf) <- c("A", "B", "C")
knitr::kable(mconf, caption = "Matrice de confusion dont les colonnes représentent la classification par ordinateur et les lignes la classification manuelle.")
```

Sur base de la matrice de confusion ci-dessus, calculez la **précision** du groupe B.

```{r conf4_h2, exercise = TRUE}
tp <- ___
fp <- ___
fn <- ___
tn <- ___
# calcul de la métrique
conf <- ___
conf
```

```{r conf4_h2-hint-1}
tp <- 10
fp <- 150
fn <- 30
tn <- 260
# calcul de la métrique
conf <- ___
conf
## Attention, le prochain indice est la solution ##
```

```{r conf4_h2-solution}
## Solution ##
tp <- 10
fp <- 150
fn <- 30
tn <- 260
# calcul de la métrique
conf <- tp / (tp + fp)
conf
```

```{r conf4_h2-check}
grade_result(
  pass_if(~ identical(.result, (10/(10+150))), "Ne pas se tromper car cette fois la référence est l'ensemble des items classés par l'ordinateur comme B, soit les vrais et les faux positifs.")
)
```

## Conclusion

```{r comm_noscore, echo=FALSE}
question_text(
  "Laissez-nous vos impressions sur cet outil pédagogique",
  answer("", TRUE, message = "Pas de commentaires... C'est bien aussi."),
  incorrect = "Vos commentaires sont enregistrés.",
  placeholder = "Entrez vos commentaires ici...",
  allow_retry = TRUE
)
```
